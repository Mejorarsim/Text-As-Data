{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 - Dataset and Pre-Processing"
      ],
      "metadata": {
        "id": "1D3-RfIuDQ4q"
      },
      "id": "1D3-RfIuDQ4q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Start by downloading the train, validation, and test splits of a version of the WikiQA corpus with the link above. Load the data into your Notebook and answer the following questions about it. Note that the dataâ€™s accompanying\n"
      ],
      "metadata": {
        "id": "O1SQTGeDDY1N"
      },
      "id": "O1SQTGeDDY1N"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f839a728",
      "metadata": {
        "id": "f839a728"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        "\n",
        "# Load the data\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        dataset = json.load(file)\n",
        "    return dataset\n",
        "\n",
        "# Load datasets\n",
        "training_set = load_dataset('/content/coursework_dataset/train.json')\n",
        "validation_set = load_dataset('/content/coursework_dataset/val.json')\n",
        "testing_set = load_dataset('/content/coursework_dataset/test.json')\n",
        "\n",
        "# Initialize SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Tokenization function\n",
        "def spacy_tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to count questions and options\n",
        "def count_items(data):\n",
        "    num_questions = len(data)\n",
        "    num_options = sum(len(item[\"options\"]) for item in data)\n",
        "    return num_questions, num_options\n",
        "\n",
        "# Initialize lists to store the counts\n",
        "dataset_names = [\"Training\", \"Validation\", \"Test\"]\n",
        "q_counts = []\n",
        "opt_counts = []\n",
        "\n",
        "# Calculate and store the counts\n",
        "for dataset_name, dataset in zip(dataset_names, [training_set, validation_set, testing_set]):\n",
        "    q_count, opt_count = count_items(dataset)\n",
        "    q_counts.append(q_count)\n",
        "    opt_counts.append(opt_count)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {\n",
        "    \"Dataset\": dataset_names,\n",
        "    \"Questions\": q_counts,\n",
        "    \"Options\": opt_counts\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Format the DataFrame\n",
        "styled_df = df.style.set_caption(\"Dataset Summary\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [('font-size', '16px'), ('font-weight', 'bold')]\n",
        "}]).applymap(lambda x: 'font-weight: bold' if x == 'Dataset' else '')\n",
        "\n",
        "# Display the styled DataFrame\n",
        "styled_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "v7T6SZL4ejQc",
        "outputId": "8ce4cd4c-ac85-4a11-8f8d-988eda8364a5"
      },
      "id": "v7T6SZL4ejQc",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x791046d90700>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ea6c4 caption {\n",
              "  font-size: 16px;\n",
              "  font-weight: bold;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ea6c4\" class=\"dataframe\">\n",
              "  <caption>Dataset Summary</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ea6c4_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
              "      <th id=\"T_ea6c4_level0_col1\" class=\"col_heading level0 col1\" >Questions</th>\n",
              "      <th id=\"T_ea6c4_level0_col2\" class=\"col_heading level0 col2\" >Options</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ea6c4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_ea6c4_row0_col0\" class=\"data row0 col0\" >Training</td>\n",
              "      <td id=\"T_ea6c4_row0_col1\" class=\"data row0 col1\" >741</td>\n",
              "      <td id=\"T_ea6c4_row0_col2\" class=\"data row0 col2\" >2964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea6c4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_ea6c4_row1_col0\" class=\"data row1 col0\" >Validation</td>\n",
              "      <td id=\"T_ea6c4_row1_col1\" class=\"data row1 col1\" >103</td>\n",
              "      <td id=\"T_ea6c4_row1_col2\" class=\"data row1 col2\" >412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ea6c4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_ea6c4_row2_col0\" class=\"data row2 col0\" >Test</td>\n",
              "      <td id=\"T_ea6c4_row2_col1\" class=\"data row2 col1\" >202</td>\n",
              "      <td id=\"T_ea6c4_row2_col2\" class=\"data row2 col2\" >808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009a602b",
      "metadata": {
        "id": "009a602b"
      },
      "source": [
        "###  (1.2) What is the average number of tokens per question in the training set? [1 mark]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "aa6c0c68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aa6c0c68",
        "outputId": "4cead4b9-647d-4e21-fdcf-d204a783ffe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of tokens per question in the training set: 6.27\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate average tokens in questions\n",
        "\n",
        "def avg_tokens_questions(data):\n",
        "    total_tokens = sum(len(spacy_tokenize(item[\"question\"])) for item in data)\n",
        "    avg_tokens = total_tokens / len(data)\n",
        "    return avg_tokens\n",
        "\n",
        "# Calculate and print the average number of tokens per question in the training set\n",
        "average_tokens_training_questions = avg_tokens_questions(training_set)\n",
        "print(f\"Average number of tokens per question in the training set: {average_tokens_training_questions:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6407734",
      "metadata": {
        "id": "c6407734"
      },
      "source": [
        "###  (1.3) What is the average number of tokens per choice in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "247181ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "247181ca",
        "outputId": "c4d1397c-85f1-483a-fa30-1f9ec88b96c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of tokens per choice in the training set: 22.34\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate average tokens in options\n",
        "\n",
        "def avg_tokens_options(data):\n",
        "    total_tokens = sum(len(spacy_tokenize(option)) for item in data for option in item[\"options\"])\n",
        "    total_options = sum(len(item[\"options\"]) for item in data)\n",
        "    avg_tokens = total_tokens / total_options\n",
        "    return avg_tokens\n",
        "\n",
        "# Calculate and print the average number of tokens per choice in the training set\n",
        "average_tokens_training_options = avg_tokens_options(training_set)\n",
        "print(f\"Average number of tokens per choice in the training set: {average_tokens_training_options:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29ba57d",
      "metadata": {
        "id": "f29ba57d"
      },
      "source": [
        "###  (1.4) What is the average number of tokens per correct choice in the training set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "23f03588",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "23f03588",
        "outputId": "197c5dc8-7ce6-44e5-d99e-3d526284ddab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of tokens per correct choice in the training set: 26.03\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate average tokens in correct options\n",
        "def avg_tokens_correct_options(data):\n",
        "    correct_indices = (item[\"correct_index\"] for item in data if \"correct_index\" in item)\n",
        "    total_tokens = sum(len(spacy_tokenize(data[i][\"options\"][index])) for i, index in enumerate(correct_indices))\n",
        "    avg_tokens = total_tokens / len(data)\n",
        "    return avg_tokens\n",
        "\n",
        "# Calculate and print the average number of tokens per correct choice in the training set\n",
        "average_tokens_correct_option = avg_tokens_correct_options(training_set)\n",
        "print(f\"Average number of tokens per correct choice in the training set: {average_tokens_correct_option:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a1e7d1d",
      "metadata": {
        "id": "0a1e7d1d"
      },
      "source": [
        "### 1.5 Perform any additional exploration of the data that you feel would be helpful for this multiple-choice question-answering task. Briefly describe what you found.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6a1ebd55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6a1ebd55",
        "outputId": "8342b8fe-4a3b-4b4b-c4b7-504a6c4c6a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# SpaCy for vectorization\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b3205950",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b3205950",
        "outputId": "7eea990b-0415-461a-95cb-869cad2477c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words in the training set: [('the', 5274), ('be', 2972), ('of', 2671), ('and', 2097), ('in', 1884), ('a', 1796), ('to', 1113), ('as', 722), ('by', 618), ('or', 500)]\n"
          ]
        }
      ],
      "source": [
        "# 1. Word Frequency Analysis\n",
        "def word_frequency(data):\n",
        "    all_tokens = [token for item in data for token in spacy_tokenize(item[\"question\"] + ' ' + ' '.join(item[\"options\"]))]\n",
        "    return Counter(all_tokens)\n",
        "\n",
        "word_freq_training = word_frequency(training_set)\n",
        "print(f\"Most common words in the training set: {word_freq_training.most_common(10)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n6pvEzeocASg",
        "outputId": "b151855c-3625-4210-bf86-e67ec62b3e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of correct options: 26.03\n",
            "Average length of incorrect options: 21.11\n"
          ]
        }
      ],
      "source": [
        "# 2. Correct Option Length vs. Incorrect Options\n",
        "def option_length_comparison(data):\n",
        "    correct_lengths = []\n",
        "    incorrect_lengths = []\n",
        "    for item in data:\n",
        "        if \"correct_index\" in item:\n",
        "            correct_lengths.append(len(spacy_tokenize(item[\"options\"][item[\"correct_index\"]])))\n",
        "            incorrect_lengths.extend(len(spacy_tokenize(option)) for idx, option in enumerate(item[\"options\"]) if idx != item[\"correct_index\"])\n",
        "    return correct_lengths, incorrect_lengths\n",
        "\n",
        "correct_lengths, incorrect_lengths = option_length_comparison(training_set)\n",
        "print(f\"Average length of correct options: {np.mean(correct_lengths):.2f}\")\n",
        "print(f\"Average length of incorrect options: {np.mean(incorrect_lengths):.2f}\")\n"
      ],
      "id": "n6pvEzeocASg"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bcc9cd39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bb78a819-5092-471b-c1e6-371ad91da94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of overlapping tokens between questions and correct options: 2.92\n"
          ]
        }
      ],
      "source": [
        "# 3. Overlap Between Questions and Correct Options\n",
        "def overlap_question_correct_option(data):\n",
        "    overlaps = []\n",
        "    for item in data:\n",
        "        if \"correct_index\" in item:\n",
        "            question_tokens = set(spacy_tokenize(item[\"question\"]))\n",
        "            correct_option_tokens = set(spacy_tokenize(item[\"options\"][item[\"correct_index\"]]))\n",
        "            overlap = question_tokens.intersection(correct_option_tokens)\n",
        "            overlaps.append(len(overlap))\n",
        "    return overlaps\n",
        "\n",
        "overlaps_training = overlap_question_correct_option(training_set)\n",
        "print(f\"Average number of overlapping tokens between questions and correct options: {np.mean(overlaps_training):.2f}\")\n"
      ],
      "id": "bcc9cd39"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d052a586",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d052a586",
        "outputId": "8433f3bc-531a-451a-a37a-c18dd629a5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-36d20bcebc9e>:10: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = question.similarity(correct_option)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average semantic similarity between questions and correct options in the training set: 0.32\n"
          ]
        }
      ],
      "source": [
        "# 4. Semantic Similarity\n",
        "\n",
        "# Now define the semantic similarity function\n",
        "def semantic_similarity(data):\n",
        "    similarities = []\n",
        "    for item in data:\n",
        "        if \"correct_index\" in item:\n",
        "            question = nlp(item[\"question\"])\n",
        "            correct_option = nlp(item[\"options\"][item[\"correct_index\"]])\n",
        "            similarity = question.similarity(correct_option)\n",
        "            similarities.append(similarity)\n",
        "    return similarities\n",
        "\n",
        "# Calculate semantic similarities for the training set\n",
        "similarities_training = semantic_similarity(training_set)\n",
        "\n",
        "# Compute the average similarity score\n",
        "average_similarity = sum(similarities_training) / len(similarities_training)\n",
        "print(f\"Average semantic similarity between questions and correct options in the training set: {average_similarity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4de0849",
      "metadata": {
        "id": "e4de0849"
      },
      "source": [
        "##  Q2-Set Similarity Measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2.1) Report the performance of each similarity measure (overlap coefficient, Sorensen-Dice & Jaccard) on the training and validation sets by measuring accuracy.\n",
        "\n",
        "(2.2) For each similarity measure, how many times was the score of the most similar answer tied with another answer? When there was a tied score among the top answers, how did you choose which to select? Why?\n"
      ],
      "metadata": {
        "id": "qyF0AM0NNuun"
      },
      "id": "qyF0AM0NNuun"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "86f709bd",
      "metadata": {
        "scrolled": true,
        "id": "86f709bd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def overlap_coefficient(set1, set2):\n",
        "    return len(set1.intersection(set2)) / min(len(set1), len(set2))\n",
        "\n",
        "def sorensen_dice_coefficient(set1, set2):\n",
        "    return 2 * len(set1.intersection(set2)) / (len(set1) + len(set2))\n",
        "\n",
        "def jaccard_similarity(set1, set2):\n",
        "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
        "\n",
        "def calculate_accuracy(data, similarity_function):\n",
        "    correct = 0\n",
        "    ties = 0\n",
        "    for item in data:\n",
        "        question_tokens = set(spacy_tokenize(item[\"question\"]))\n",
        "        scores = [similarity_function(question_tokens, set(spacy_tokenize(option))) for option in item[\"options\"]]\n",
        "        max_score = max(scores)\n",
        "        if scores.count(max_score) > 1:  # Check if there's a tie\n",
        "            ties += 1\n",
        "            # Implement tie-breaking strategy here. For now, we pick the first option with the highest score.\n",
        "            chosen_index = scores.index(max_score)\n",
        "        else:\n",
        "            chosen_index = scores.index(max_score)\n",
        "        if chosen_index == item[\"correct_index\"]:\n",
        "            correct += 1\n",
        "    accuracy = correct / len(data)\n",
        "    return accuracy, ties\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Accuracy and tie information for training set\n",
        "training_data = {\n",
        "    \"Similarity Measure\": [\"Overlap\", \"Sorensen-Dice\", \"Jaccard\"],\n",
        "    \"Accuracy\": [train_accuracy_overlap, train_accuracy_sorensen, train_accuracy_jaccard],\n",
        "    \"Ties\": [train_ties_overlap, train_ties_sorensen, train_ties_jaccard]\n",
        "}\n",
        "\n",
        "# Accuracy and tie information for validation set\n",
        "validation_data = {\n",
        "    \"Similarity Measure\": [\"Overlap\", \"Sorensen-Dice\", \"Jaccard\"],\n",
        "    \"Accuracy\": [val_accuracy_overlap, val_accuracy_sorensen, val_accuracy_jaccard],\n",
        "    \"Ties\": [val_ties_overlap, val_ties_sorensen, val_ties_jaccard]\n",
        "}\n",
        "\n",
        "# Create DataFrames\n",
        "df_training = pd.DataFrame(training_data)\n",
        "df_validation = pd.DataFrame(validation_data)\n",
        "\n",
        "# Apply styling\n",
        "df_training_styled = df_training.style.set_caption(\"Training Set\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Accuracy', 'Ties'])\n",
        "\n",
        "df_validation_styled = df_validation.style.set_caption(\"Validation Set\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Accuracy', 'Ties'])\n",
        "\n",
        "# Display styled DataFrames\n",
        "display(df_training_styled)\n",
        "display(df_validation_styled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "oPy_RAOyVDeR",
        "outputId": "744075ee-98e6-4004-db46-b12be76c2d83"
      },
      "id": "oPy_RAOyVDeR",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x790fa558d4e0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e516a caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_e516a_row0_col1, #T_e516a_row0_col2 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_e516a_row1_col1, #T_e516a_row1_col2, #T_e516a_row2_col1, #T_e516a_row2_col2 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e516a\" class=\"dataframe\">\n",
              "  <caption>Training Set</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e516a_level0_col0\" class=\"col_heading level0 col0\" >Similarity Measure</th>\n",
              "      <th id=\"T_e516a_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_e516a_level0_col2\" class=\"col_heading level0 col2\" >Ties</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e516a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_e516a_row0_col0\" class=\"data row0 col0\" >Overlap</td>\n",
              "      <td id=\"T_e516a_row0_col1\" class=\"data row0 col1\" >0.527665</td>\n",
              "      <td id=\"T_e516a_row0_col2\" class=\"data row0 col2\" >258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e516a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_e516a_row1_col0\" class=\"data row1 col0\" >Sorensen-Dice</td>\n",
              "      <td id=\"T_e516a_row1_col1\" class=\"data row1 col1\" >0.433198</td>\n",
              "      <td id=\"T_e516a_row1_col2\" class=\"data row1 col2\" >18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e516a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_e516a_row2_col0\" class=\"data row2 col0\" >Jaccard</td>\n",
              "      <td id=\"T_e516a_row2_col1\" class=\"data row2 col1\" >0.433198</td>\n",
              "      <td id=\"T_e516a_row2_col2\" class=\"data row2 col2\" >18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x790fa558cb20>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a9c05 caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_a9c05_row0_col1, #T_a9c05_row0_col2 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_a9c05_row1_col1, #T_a9c05_row1_col2, #T_a9c05_row2_col1, #T_a9c05_row2_col2 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a9c05\" class=\"dataframe\">\n",
              "  <caption>Validation Set</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a9c05_level0_col0\" class=\"col_heading level0 col0\" >Similarity Measure</th>\n",
              "      <th id=\"T_a9c05_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_a9c05_level0_col2\" class=\"col_heading level0 col2\" >Ties</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a9c05_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a9c05_row0_col0\" class=\"data row0 col0\" >Overlap</td>\n",
              "      <td id=\"T_a9c05_row0_col1\" class=\"data row0 col1\" >0.504854</td>\n",
              "      <td id=\"T_a9c05_row0_col2\" class=\"data row0 col2\" >33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9c05_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a9c05_row1_col0\" class=\"data row1 col0\" >Sorensen-Dice</td>\n",
              "      <td id=\"T_a9c05_row1_col1\" class=\"data row1 col1\" >0.368932</td>\n",
              "      <td id=\"T_a9c05_row1_col2\" class=\"data row1 col2\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9c05_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a9c05_row2_col0\" class=\"data row2 col0\" >Jaccard</td>\n",
              "      <td id=\"T_a9c05_row2_col1\" class=\"data row2 col1\" >0.368932</td>\n",
              "      <td id=\"T_a9c05_row2_col2\" class=\"data row2 col2\" >2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3-Cosine similarity of TF vectors"
      ],
      "metadata": {
        "id": "oo4lk2CdYscG"
      },
      "id": "oo4lk2CdYscG"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Generate term frequency (TF) vectors of each question as well as the four possible answers. You should use the CountVectorizer with default settings (but use the same tokenizer as in Q1 and Q2). For each question, pick the answer with the highest cosine similarity between its TF vector and the question's TF vector.\n",
        "\n",
        " (3.1) Report the performance of the training and validation sets by measuring accuracy. Discuss how they compare\n",
        " with the set similarity measures from Q2. [6 marks]"
      ],
      "metadata": {
        "id": "qQBfkpZiYoVI"
      },
      "id": "qQBfkpZiYoVI"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "uUmRQknipIvl"
      },
      "id": "uUmRQknipIvl",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy English tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Custom tokenizer function\n",
        "def text_pipeline_spacy_special(text):\n",
        "    # Tokenize the text using SpaCy\n",
        "    doc = nlp(text)\n",
        "    # Extract lemmatized tokens and remove punctuation and whitespace\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "e7bhJ6oxuLbq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3afb5007-6809-492a-bd53-ac7291cb5ad6"
      },
      "id": "e7bhJ6oxuLbq",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Code for calculating cosine similarity and best answer\n",
        "def calculate_cosine_similarity_and_best_answer(data, tf_question_vectors, tf_answer_vectors):\n",
        "    correct_predictions = 0\n",
        "    for i, item in enumerate(data):\n",
        "        question_vector = tf_question_vectors[i:i+1]  # This slices the question vector correctly for cosine_similarity\n",
        "        answers_start = i * 4\n",
        "        answers_end = answers_start + 4\n",
        "        answer_vectors = tf_answer_vectors[answers_start:answers_end]  # This slices the answer vectors correctly\n",
        "        similarity_scores = cosine_similarity(question_vector, answer_vectors)\n",
        "        selected_answer_index = similarity_scores.argmax()\n",
        "        if selected_answer_index == item['correct_index']:\n",
        "            correct_predictions += 1\n",
        "    accuracy = correct_predictions / len(data)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "6tEEZOumpHY5"
      },
      "id": "6tEEZOumpHY5",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Evaluation with TF-IDF\n",
        "train_question_vectors_tfidf, train_answer_vectors_tfidf = generate_tfidf_vectors(training_set)\n",
        "valid_question_vectors_tfidf, valid_answer_vectors_tfidf = generate_tfidf_vectors(validation_set)\n",
        "train_cosine_similarity_accuracy_tfidf = calculate_cosine_similarity_and_best_answer(training_set, train_question_vectors_tfidf, train_answer_vectors_tfidf)\n",
        "valid_cosine_similarity_accuracy_tfidf = calculate_cosine_similarity_and_best_answer(validation_set, valid_question_vectors_tfidf, valid_answer_vectors_tfidf)\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_data = {\n",
        "    \"Dataset\": [\"Training\", \"Validation\"],\n",
        "    \"Size\": [len(training_set), len(validation_set)],\n",
        "    \"Cosine Similarity Accuracy\": [train_cosine_similarity_accuracy_tfidf, valid_cosine_similarity_accuracy_tfidf]\n",
        "}\n",
        "df_results = pd.DataFrame(results_data)\n",
        "\n",
        "# Apply styling\n",
        "df_results_styled = df_results.style.set_caption(\"Results for TF-IDF Weighting\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Cosine Similarity Accuracy'])\n",
        "\n",
        "# Display styled DataFrame\n",
        "display(df_results_styled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "83q_bVDaT4jq",
        "outputId": "40d1af8d-e34c-408a-b51b-02b3a0211ad6"
      },
      "id": "83q_bVDaT4jq",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x790fa5991690>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_187a7 caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_187a7_row0_col2 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_187a7_row1_col2 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_187a7\" class=\"dataframe\">\n",
              "  <caption>Results for TF-IDF Weighting</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_187a7_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
              "      <th id=\"T_187a7_level0_col1\" class=\"col_heading level0 col1\" >Size</th>\n",
              "      <th id=\"T_187a7_level0_col2\" class=\"col_heading level0 col2\" >Cosine Similarity Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_187a7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_187a7_row0_col0\" class=\"data row0 col0\" >Training</td>\n",
              "      <td id=\"T_187a7_row0_col1\" class=\"data row0 col1\" >741</td>\n",
              "      <td id=\"T_187a7_row0_col2\" class=\"data row0 col2\" >0.431849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_187a7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_187a7_row1_col0\" class=\"data row1 col0\" >Validation</td>\n",
              "      <td id=\"T_187a7_row1_col1\" class=\"data row1 col1\" >103</td>\n",
              "      <td id=\"T_187a7_row1_col2\" class=\"data row1 col2\" >0.407767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Generating TF vectors and calculate cosine similarity and accuracy for training and validation sets\n",
        "train_question_vectors_tf, train_answer_vectors_tf = generate_tf_vectors(training_set)\n",
        "valid_question_vectors_tf, valid_answer_vectors_tf = generate_tf_vectors(validation_set)\n",
        "cosine_similar_accuracy_train = calculate_cosine_similarity_and_best_answer(training_set, train_question_vectors_tf, train_answer_vectors_tf)\n",
        "cosine_similar_accuracy_val = calculate_cosine_similarity_and_best_answer(validation_set, valid_question_vectors_tf, valid_answer_vectors_tf)\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_data_tf = {\n",
        "    \"Dataset\": [\"Training\", \"Validation\"],\n",
        "    \"Size\": [len(training_set), len(validation_set)],\n",
        "    \"Cosine Similarity Accuracy\": [cosine_similar_accuracy_train, cosine_similar_accuracy_val]\n",
        "}\n",
        "df_results_tf = pd.DataFrame(results_data_tf)\n",
        "\n",
        "# Apply styling\n",
        "df_results_tf_styled = df_results_tf.style.set_caption(\"Results for TF Vector\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Cosine Similarity Accuracy'])\n",
        "\n",
        "# Display styled DataFrame\n",
        "display(df_results_tf_styled)\n"
      ],
      "metadata": {
        "id": "Lw-qrMZm_ZgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "99d9dac1-fc94-4238-98f5-38bc561d29bf"
      },
      "id": "Lw-qrMZm_ZgA",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7910409d5630>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_1ffb6 caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_1ffb6_row0_col2 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_1ffb6_row1_col2 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_1ffb6\" class=\"dataframe\">\n",
              "  <caption>Results for TF Vector</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_1ffb6_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
              "      <th id=\"T_1ffb6_level0_col1\" class=\"col_heading level0 col1\" >Size</th>\n",
              "      <th id=\"T_1ffb6_level0_col2\" class=\"col_heading level0 col2\" >Cosine Similarity Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_1ffb6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_1ffb6_row0_col0\" class=\"data row0 col0\" >Training</td>\n",
              "      <td id=\"T_1ffb6_row0_col1\" class=\"data row0 col1\" >741</td>\n",
              "      <td id=\"T_1ffb6_row0_col2\" class=\"data row0 col2\" >0.449393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_1ffb6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_1ffb6_row1_col0\" class=\"data row1 col0\" >Validation</td>\n",
              "      <td id=\"T_1ffb6_row1_col1\" class=\"data row1 col1\" >103</td>\n",
              "      <td id=\"T_1ffb6_row1_col2\" class=\"data row1 col2\" >0.436893</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4-Cosine similarity of vectors from bert-base-uncased\n"
      ],
      "metadata": {
        "id": "1GZnLduPy39K"
      },
      "id": "1GZnLduPy39K"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use the feature-extraction pipeline with a bert-based-uncased model to create context vectors from the bert-based-uncased model for the text of each question and its four answers separately. You should use the context vector that represents the [CLS] token, which will be the first vector. For each question, pick the answer with the highest cosine similarity between its vector and the questionâ€™s vector.\n",
        "\n",
        " (4.1) Report the performance of the training and validation sets by measuring accuracy.\n",
        "\n",
        " (4.2) What are the limitations of the set similarity and cosine similarity methods used in Q2, Q3 and Q4?"
      ],
      "metadata": {
        "id": "_JFdIeWry-0r"
      },
      "id": "_JFdIeWry-0r"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fdbe92a4",
      "metadata": {
        "id": "fdbe92a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "242cad80-2e3b-4a64-cd8e-7da1565d9ac9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x790fcfeaf7c0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e0957 caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_e0957_row0_col1 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e0957_row1_col1 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e0957\" class=\"dataframe\">\n",
              "  <caption>Results for BERT Model</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e0957_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
              "      <th id=\"T_e0957_level0_col1\" class=\"col_heading level0 col1\" >Cosine Similarity Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e0957_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_e0957_row0_col0\" class=\"data row0 col0\" >Training</td>\n",
              "      <td id=\"T_e0957_row0_col1\" class=\"data row0 col1\" >0.143050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e0957_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_e0957_row1_col0\" class=\"data row1 col0\" >Validation</td>\n",
              "      <td id=\"T_e0957_row1_col1\" class=\"data row1 col1\" >0.203883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to extract context vectors for text\n",
        "def extract_context_vectors(text):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embeddings = outputs[0][:, 0, :]  # Extract CLS token embeddings\n",
        "    return cls_embeddings\n",
        "\n",
        "# Function to calculate cosine similarity and select the most similar answer\n",
        "def calculate_cosine_similarity_and_select_answer(data):\n",
        "    correct_predictions = 0\n",
        "    for item in data:\n",
        "        question_vector = extract_context_vectors(item['question'])\n",
        "        max_similarity_score = -1\n",
        "        selected_answer_index = -1\n",
        "        for idx, option in enumerate(item['options']):\n",
        "            answer_vector = extract_context_vectors(option)\n",
        "            similarity_score = cosine_similarity(question_vector, answer_vector)[0][0]\n",
        "            if similarity_score > max_similarity_score:\n",
        "                max_similarity_score = similarity_score\n",
        "                selected_answer_index = idx\n",
        "        # Check if the selected answer is correct\n",
        "        if selected_answer_index == item['correct_index']:\n",
        "            correct_predictions += 1\n",
        "    accuracy = correct_predictions / len(data)\n",
        "    return accuracy\n",
        "\n",
        "# Calculate cosine similarity and accuracy for training and validation sets\n",
        "cosine_similarity_accuracy_train = calculate_cosine_similarity_and_select_answer(training_set)\n",
        "cosine_similarity_accuracy_val = calculate_cosine_similarity_and_select_answer(validation_set)\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_data_bert = {\n",
        "    \"Dataset\": [\"Training\", \"Validation\"],\n",
        "    \"Cosine Similarity Accuracy\": [cosine_similarity_accuracy_train, cosine_similarity_accuracy_val]\n",
        "}\n",
        "df_results_bert = pd.DataFrame(results_data_bert)\n",
        "\n",
        "# Apply styling\n",
        "df_results_bert_styled = df_results_bert.style.set_caption(\"Results for BERT Model\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Cosine Similarity Accuracy'])\n",
        "\n",
        "# Display styled DataFrame\n",
        "display(df_results_bert_styled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5 - Fine-tuning a transformer model [18 marks]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xSLVvCQ-rMi7"
      },
      "id": "xSLVvCQ-rMi7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5.1) Report the accuracy, precision, recall and F1 score of the predictions on the question-option pairs representation of the training and validation sets."
      ],
      "metadata": {
        "id": "aPJVo0frrRuE"
      },
      "id": "aPJVo0frrRuE"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define dataset class\n",
        "class QuestionOptionDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.data = []\n",
        "        for item in dataset:\n",
        "            question = item['question']\n",
        "            options = item['options']\n",
        "            correct_index = item['correct_index']\n",
        "            for i, option in enumerate(options):\n",
        "                input_text = f\"{question} [SEP] {option}\"\n",
        "                label = 1 if i == correct_index else 0\n",
        "                self.data.append((input_text, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = QuestionOptionDataset(training_set)\n",
        "valid_dataset = QuestionOptionDataset(validation_set)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8)\n",
        "\n",
        "# Define training function\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs = tokenizer(batch[0], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        labels = torch.tensor(batch[1]).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(model, valid_loader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            inputs = tokenizer(batch[0], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "            labels = torch.tensor(batch[1]).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 1e-5\n",
        "epochs = 4\n",
        "weight_decay = 0\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "# Evaluate on the validation set\n",
        "accuracy, precision, recall, f1 = evaluate(model, valid_loader, device)\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_data_bert_classifier = {\n",
        "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
        "    \"Validation Set\": [accuracy, precision, recall, f1]\n",
        "}\n",
        "df_results_bert_classifier = pd.DataFrame(results_data_bert_classifier)\n",
        "\n",
        "# Apply styling\n",
        "df_results_bert_classifier_styled = df_results_bert_classifier.style.set_caption(\"Results for BERT Classifier Model\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Validation Set'])\n",
        "\n",
        "# Display styled DataFrame\n",
        "display(df_results_bert_classifier_styled)\n"
      ],
      "metadata": {
        "id": "JyHBqI9caa9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "e54e1f1e-60a4-4c79-ae2f-f62f43c97492"
      },
      "id": "JyHBqI9caa9W",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-44-81f7a25e2be7>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(batch[1]).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 0.5268\n",
            "Epoch 2, Train Loss: 0.4373\n",
            "Epoch 3, Train Loss: 0.3250\n",
            "Epoch 4, Train Loss: 0.2056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-81f7a25e2be7>:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(batch[1]).to(device)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x791046afa3e0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_d69c0 caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_d69c0_row0_col1 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_d69c0_row1_col1 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_d69c0_row2_col1 {\n",
              "  background-color: #25848e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_d69c0_row3_col1 {\n",
              "  background-color: #404688;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_d69c0\" class=\"dataframe\">\n",
              "  <caption>Results for BERT Classifier Model</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_d69c0_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
              "      <th id=\"T_d69c0_level0_col1\" class=\"col_heading level0 col1\" >Validation Set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_d69c0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_d69c0_row0_col0\" class=\"data row0 col0\" >Accuracy</td>\n",
              "      <td id=\"T_d69c0_row0_col1\" class=\"data row0 col1\" >0.791262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d69c0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_d69c0_row1_col0\" class=\"data row1 col0\" >Precision</td>\n",
              "      <td id=\"T_d69c0_row1_col1\" class=\"data row1 col1\" >0.570248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d69c0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_d69c0_row2_col0\" class=\"data row2 col0\" >Recall</td>\n",
              "      <td id=\"T_d69c0_row2_col1\" class=\"data row2 col1\" >0.669903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_d69c0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_d69c0_row3_col0\" class=\"data row3 col0\" >F1 Score</td>\n",
              "      <td id=\"T_d69c0_row3_col1\" class=\"data row3 col1\" >0.616071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(5.2) Report the accuracy for this method for selecting the correct answer on the training and validation sets of this model. Note this is different from the value in part (a). To enable this, select the option for each question with the highest output logit value for the positive class of the model."
      ],
      "metadata": {
        "id": "6svxam_OrgqV"
      },
      "id": "6svxam_OrgqV"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def select_correct_answer(model, loader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_questions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            inputs = tokenizer(batch[0], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "            labels = torch.tensor(batch[1]).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels).item()\n",
        "            total_questions += labels.size(0)\n",
        "    accuracy = correct_predictions / total_questions\n",
        "    return accuracy\n",
        "\n",
        "# Calculate accuracy for selecting the correct answer on the training set\n",
        "train_accuracy_selecting_correct_answer = select_correct_answer(model, train_loader, device)\n",
        "\n",
        "# Calculate accuracy for selecting the correct answer on the validation set\n",
        "valid_accuracy_selecting_correct_answer = select_correct_answer(model, valid_loader, device)\n",
        "\n",
        "# Create DataFrame for results\n",
        "results_data_selecting_correct_answer = {\n",
        "    \"Dataset\": [\"Training\", \"Validation\"],\n",
        "    \"Accuracy for Selecting Correct Answer\": [train_accuracy_selecting_correct_answer, valid_accuracy_selecting_correct_answer]\n",
        "}\n",
        "df_results_selecting_correct_answer = pd.DataFrame(results_data_selecting_correct_answer)\n",
        "\n",
        "# Apply styling\n",
        "df_results_selecting_correct_answer_styled = df_results_selecting_correct_answer.style.set_caption(\"Accuracy for Selecting the Correct Answer\").set_table_styles([{\n",
        "    'selector': 'caption',\n",
        "    'props': [\n",
        "        ('color', 'blue'),\n",
        "        ('font-size', '16px')\n",
        "    ]\n",
        "}]).background_gradient(cmap='viridis', subset=['Accuracy for Selecting Correct Answer'])\n",
        "\n",
        "# Display styled DataFrame\n",
        "display(df_results_selecting_correct_answer_styled)\n"
      ],
      "metadata": {
        "id": "FUDZMe7_bK7u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "9eeb8d24-2cb6-4ecd-e0a0-b2a6ed8d07b5"
      },
      "id": "FUDZMe7_bK7u",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-1a6256406498>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(batch[1]).to(device)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x791042219ea0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_2acdc caption {\n",
              "  color: blue;\n",
              "  font-size: 16px;\n",
              "}\n",
              "#T_2acdc_row0_col1 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_2acdc_row1_col1 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_2acdc\" class=\"dataframe\">\n",
              "  <caption>Accuracy for Selecting the Correct Answer</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_2acdc_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
              "      <th id=\"T_2acdc_level0_col1\" class=\"col_heading level0 col1\" >Accuracy for Selecting Correct Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_2acdc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_2acdc_row0_col0\" class=\"data row0 col0\" >Training</td>\n",
              "      <td id=\"T_2acdc_row0_col1\" class=\"data row0 col1\" >0.958165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2acdc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_2acdc_row1_col0\" class=\"data row1 col0\" >Validation</td>\n",
              "      <td id=\"T_2acdc_row1_col1\" class=\"data row1 col1\" >0.791262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Thank you*"
      ],
      "metadata": {
        "id": "ALhC14lRrnEG"
      },
      "id": "ALhC14lRrnEG"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCk8JAaxrrq_"
      },
      "id": "OCk8JAaxrrq_",
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}